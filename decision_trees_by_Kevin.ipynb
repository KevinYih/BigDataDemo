{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KevinYih/BigDataDemo/blob/main/decision_trees_by_Kevin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0-YhEpP_Ds-"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zsj5WYpR9QId"
      },
      "source": [
        "Let's set up Spark on your Colab environment.  Run the cell below!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-qHai2252mI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "0f3c4e2b-ee08-4425-aaa7-5311e16d4185"
      },
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=5f4b183db0f71fe31dbaa6954348ce0834cc6017b45b206819e55e2a9f8a594d\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n",
            "The following additional packages will be installed:\n",
            "  libxtst6 openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra fonts-nanum fonts-ipafont-gothic\n",
            "  fonts-ipafont-mincho fonts-wqy-microhei fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  libxtst6 openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 3 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 39.6 MB of archives.\n",
            "After this operation, 144 MB of additional disk space will be used.\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "(Reading database ... 121913 files and directories currently installed.)\n",
            "Preparing to unpack .../libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u412-ga-1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u412-ga-1~22.04.1) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u412-ga-1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u412-ga-1~22.04.1) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u412-ga-1~22.04.1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u412-ga-1~22.04.1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwtlO4_m_LbQ"
      },
      "source": [
        "Now we import some of the libraries usually needed by our workload.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twk-K-jilWK7"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtrJlMBt1Ela"
      },
      "source": [
        "Let's initialize the Spark context."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create the session\n",
        "conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n",
        "\n",
        "# create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "KH91tEik9zZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAYRX2PMm0L6"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hXdMR6wnEIM"
      },
      "source": [
        "In this Colab, rather than downloading a file from Google Drive, we will load a famous machine learning dataset, the [Breast Cancer Wisconsin dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html), using the ```scikit-learn``` datasets loader."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5K93ABEy9Zlo"
      },
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "breast_cancer = load_breast_cancer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpsaYOqRxar2"
      },
      "source": [
        "For convenience, given that the dataset is small, we first\n",
        "\n",
        "*   construct a Pandas dataframe\n",
        "*   tune the schema\n",
        "*   and convert it into a Spark dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oitav_xhQD9w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "55621b19-0570-48ed-b7fa-69133c7063b6"
      },
      "source": [
        "pd_df = pd.DataFrame(breast_cancer.data, columns=breast_cancer.feature_names)\n",
        "df = spark.createDataFrame(pd_df)\n",
        "\n",
        "def set_df_columns_nullable(spark, df, column_list, nullable=False):\n",
        "    for struct_field in df.schema:\n",
        "        if struct_field.name in column_list:\n",
        "            struct_field.nullable = nullable\n",
        "    df_mod = spark.createDataFrame(df.rdd, df.schema)\n",
        "    return df_mod\n",
        "\n",
        "df = set_df_columns_nullable(spark, df, df.columns)\n",
        "df = df.withColumn('features', array(df.columns))\n",
        "vectors = df.rdd.map(lambda row: Vectors.dense(row.features))\n",
        "\n",
        "df.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- mean radius: double (nullable = false)\n",
            " |-- mean texture: double (nullable = false)\n",
            " |-- mean perimeter: double (nullable = false)\n",
            " |-- mean area: double (nullable = false)\n",
            " |-- mean smoothness: double (nullable = false)\n",
            " |-- mean compactness: double (nullable = false)\n",
            " |-- mean concavity: double (nullable = false)\n",
            " |-- mean concave points: double (nullable = false)\n",
            " |-- mean symmetry: double (nullable = false)\n",
            " |-- mean fractal dimension: double (nullable = false)\n",
            " |-- radius error: double (nullable = false)\n",
            " |-- texture error: double (nullable = false)\n",
            " |-- perimeter error: double (nullable = false)\n",
            " |-- area error: double (nullable = false)\n",
            " |-- smoothness error: double (nullable = false)\n",
            " |-- compactness error: double (nullable = false)\n",
            " |-- concavity error: double (nullable = false)\n",
            " |-- concave points error: double (nullable = false)\n",
            " |-- symmetry error: double (nullable = false)\n",
            " |-- fractal dimension error: double (nullable = false)\n",
            " |-- worst radius: double (nullable = false)\n",
            " |-- worst texture: double (nullable = false)\n",
            " |-- worst perimeter: double (nullable = false)\n",
            " |-- worst area: double (nullable = false)\n",
            " |-- worst smoothness: double (nullable = false)\n",
            " |-- worst compactness: double (nullable = false)\n",
            " |-- worst concavity: double (nullable = false)\n",
            " |-- worst concave points: double (nullable = false)\n",
            " |-- worst symmetry: double (nullable = false)\n",
            " |-- worst fractal dimension: double (nullable = false)\n",
            " |-- features: array (nullable = false)\n",
            " |    |-- element: double (containsNull = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtR1xRvonxiO"
      },
      "source": [
        "With the next cell, we build the two data structures that we will be using throughout this Colab:\n",
        "\n",
        "\n",
        "*   ```features```, a dataframe of Dense vectors, containing all the original features in the dataset;\n",
        "*   ```labels```, a series of binary labels indicating if the corresponding set of features belongs to a subject with breast cancer, or not.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GP23Xkgwi0SD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d47e294a-a01d-4c97-e55e-b432aa4896ba"
      },
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "features = spark.createDataFrame(vectors.map(Row), [\"features\"])\n",
        "labels = pd.Series(breast_cancer.target)\n",
        "\n",
        "features.show(9)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|            features|\n",
            "+--------------------+\n",
            "|[17.99,10.38,122....|\n",
            "|[20.57,17.77,132....|\n",
            "|[19.69,21.25,130....|\n",
            "|[11.42,20.38,77.5...|\n",
            "|[20.29,14.34,135....|\n",
            "|[12.45,15.7,82.57...|\n",
            "|[18.25,19.98,119....|\n",
            "|[13.71,20.83,90.2...|\n",
            "|[13.0,21.82,87.5,...|\n",
            "+--------------------+\n",
            "only showing top 9 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRaF2A_j_nC7"
      },
      "source": [
        "### Your task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebLNUxP0_8x3"
      },
      "source": [
        "If you run successfully the Setup and Data Preprocessing stages, you are now ready to use decision tree library of pyspark ([documentation](https://spark.apache.org/docs/1.5.2/ml-decision-tree.html)) to predict labels. In the documentation look for the python code example.\n",
        "\n",
        "Start by trying to understand the nature of the dataset that you are given. Then, investigate what are the purposes of `StringIndexer`, `VectorIndexer`, and `Pipeline` in the example provided in the documentation.  Do you have to use them for your dataset?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xVIfPHZwWaE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c4e83e3-deba-4277-a8ae-a6a96aba7953"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.mllib.util import MLUtils\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "from pyspark.ml.linalg import Vectors\n",
        "import pandas as pd\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "# Index labels, adding metadata to the label column.\n",
        "# Fit on whole dataset to include all labels in index.\n",
        "\n",
        "\n",
        "# convert pandas Series to DataFrame，and name it 'label'\n",
        "labels_df = spark.createDataFrame(labels.reset_index().rename(columns={0: 'label'}))\n",
        "\n",
        "# add index\n",
        "features_with_index = features.withColumn(\"index\", F.monotonically_increasing_id())\n",
        "labels_with_index = labels_df.withColumn(\"index\", F.monotonically_increasing_id())\n",
        "\n",
        "# join features and labels DataFrame\n",
        "data = features_with_index.join(labels_with_index, on=\"index\").drop(\"index\")\n",
        "\n",
        "# show the data DataFrame\n",
        "data.show(5)\n",
        "\n",
        "# Index labels, adding metadata to the label column.\n",
        "# Fit on whole dataset to include all labels in index.\n",
        "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(data)\n",
        "# Automatically identify categorical features, and index them.\n",
        "# We specify maxCategories so features with > 4 distinct values are treated as continuous.\n",
        "featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
        "\n",
        "# Split the data into training and test sets (30% held out for testing)\n",
        "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
        "\n",
        "# Train a DecisionTree model.\n",
        "dt = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\n",
        "\n",
        "# Chain indexers and tree in a Pipeline\n",
        "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, dt])\n",
        "\n",
        "# Train model.  This also runs the indexers.\n",
        "model = pipeline.fit(trainingData)\n",
        "\n",
        "# Make predictions.\n",
        "predictions = model.transform(testData)\n",
        "\n",
        "# Select example rows to display.\n",
        "predictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)\n",
        "\n",
        "# Select (prediction, true label) and compute test error\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print (\"Test Error = %g\" % (1.0 - accuracy))\n",
        "\n",
        "treeModel = model.stages[2]\n",
        "print (treeModel) # summary only"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+\n",
            "|            features|label|\n",
            "+--------------------+-----+\n",
            "|[14.58,21.53,97.4...|    0|\n",
            "|[17.57,15.05,115....|    0|\n",
            "|[14.78,23.94,97.4...|    0|\n",
            "|[12.77,21.41,82.0...|    1|\n",
            "|[10.18,17.53,65.1...|    1|\n",
            "+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "+----------+------------+--------------------+\n",
            "|prediction|indexedLabel|            features|\n",
            "+----------+------------+--------------------+\n",
            "|       0.0|         0.0|[8.196,16.84,51.7...|\n",
            "|       0.0|         0.0|[8.618,11.79,54.3...|\n",
            "|       0.0|         0.0|[8.671,14.45,54.4...|\n",
            "|       0.0|         0.0|[9.173,13.86,59.2...|\n",
            "|       0.0|         0.0|[9.436,18.32,59.8...|\n",
            "+----------+------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Test Error = 0.0662651\n",
            "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_19deb4e5b448, depth=5, numNodes=27, numClasses=2, numFeatures=30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "For the next step use the tree ensembles for predicting labels ([documentation](https://spark.apache.org/docs/1.5.2/ml-ensembles.html)).\n",
        "\n",
        "Compare the accuracy of the two approach."
      ],
      "metadata": {
        "id": "CSW5CvWtGECs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.mllib.util import MLUtils\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "from pyspark.ml.linalg import Vectors\n",
        "import pandas as pd\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "\n",
        "# convert pandas Series to DataFrame，and name it 'label'\n",
        "labels_df = spark.createDataFrame(labels.reset_index().rename(columns={0: 'label'}))\n",
        "\n",
        "# add index\n",
        "features_with_index = features.withColumn(\"index\", F.monotonically_increasing_id())\n",
        "labels_with_index = labels_df.withColumn(\"index\", F.monotonically_increasing_id())\n",
        "\n",
        "# join features and labels DataFrame\n",
        "data = features_with_index.join(labels_with_index, on=\"index\").drop(\"index\")\n",
        "\n",
        "# show the data DataFrame\n",
        "data.show(5)\n",
        "\n",
        "# Index labels, adding metadata to the label column.\n",
        "# Fit on whole dataset to include all labels in index.\n",
        "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(data)\n",
        "# Automatically identify categorical features, and index them.\n",
        "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
        "featureIndexer =\\\n",
        "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
        "\n",
        "# Split the data into training and test sets (30% held out for testing)\n",
        "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
        "\n",
        "# Train a RandomForest model.\n",
        "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\n",
        "\n",
        "# Chain indexers and forest in a Pipeline\n",
        "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf])\n",
        "\n",
        "# Train model.  This also runs the indexers.\n",
        "model = pipeline.fit(trainingData)\n",
        "\n",
        "# Make predictions.\n",
        "predictions = model.transform(testData)\n",
        "\n",
        "# Select example rows to display.\n",
        "predictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)\n",
        "\n",
        "# Select (prediction, true label) and compute test error\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print (\"Test Error = %g\" % (1.0 - accuracy))\n",
        "\n",
        "rfModel = model.stages[2]\n",
        "print (rfModel) # summary only\n"
      ],
      "metadata": {
        "id": "8DcQF6pCGDyU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b6ddf69-d938-4dda-baf0-3114f1d179bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+\n",
            "|            features|label|\n",
            "+--------------------+-----+\n",
            "|[14.58,21.53,97.4...|    0|\n",
            "|[17.57,15.05,115....|    0|\n",
            "|[14.78,23.94,97.4...|    0|\n",
            "|[12.77,21.41,82.0...|    1|\n",
            "|[10.18,17.53,65.1...|    1|\n",
            "+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "+----------+------------+--------------------+\n",
            "|prediction|indexedLabel|            features|\n",
            "+----------+------------+--------------------+\n",
            "|       0.0|         0.0|[7.691,25.44,48.3...|\n",
            "|       0.0|         0.0|[9.173,13.86,59.2...|\n",
            "|       0.0|         0.0|[9.436,18.32,59.8...|\n",
            "|       0.0|         0.0|[9.465,21.01,60.1...|\n",
            "|       0.0|         0.0|[9.683,19.34,61.0...|\n",
            "+----------+------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Test Error = 0.0337838\n",
            "RandomForestClassificationModel: uid=RandomForestClassifier_98e62b1e14fc, numTrees=20, numClasses=2, numFeatures=30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# upload csv\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "LpuVUPjUqjiq",
        "outputId": "2e57c04c-160e-4530-a5f3-b26c1d10680a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-14f508e9-3135-4612-b5a0-4e80d27ebb10\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-14f508e9-3135-4612-b5a0-4e80d27ebb10\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Admission_Predict.csv to Admission_Predict.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Option 1: Graduates Admission Prediction\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.regression import DecisionTreeRegressor\n",
        "from pyspark.ml.feature import VectorIndexer\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.mllib.util import MLUtils\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.sql import Row\n",
        "from pyspark.ml.linalg import Vectors\n",
        "import pandas as pd\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "# create SparkSession\n",
        "spark = SparkSession.builder.appName(\"Admission_Predict\").getOrCreate()\n",
        "\n",
        "# load CSV to Spark DataFrame\n",
        "file_path = \"Admission_Predict.csv\"\n",
        "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "\n",
        "df.show(5)\n",
        "\n",
        "# Specify the feature columns and the label column\n",
        "feature_columns = [\"GRE Score\", \"TOEFL Score\", \"University Rating\", \"SOP\", \"LOR \", \"CGPA\", \"Research\"]\n",
        "label_column = \"Chance of Admit \"\n",
        "\n",
        "# Use VectorAssembler to combine feature columns into a single vector column\n",
        "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
        "\n",
        "# Transform the DataFrame\n",
        "df_transformed = assembler.transform(df)\n",
        "\n",
        "# Select only the features and label columns\n",
        "data = df_transformed.select(\"features\", label_column)\n",
        "data = data.withColumnRenamed(label_column, \"label\")\n",
        "\n",
        "data.show(5)\n",
        "\n",
        "# Automatically identify categorical features, and index them.\n",
        "# We specify maxCategories so features with > 4 distinct values are treated as continuous.\n",
        "featureIndexer =\\\n",
        "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
        "\n",
        "# Split the data into training and test sets (30% held out for testing)\n",
        "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
        "\n",
        "# Train a DecisionTree model.\n",
        "dt = DecisionTreeRegressor(featuresCol=\"indexedFeatures\")\n",
        "\n",
        "# Chain indexer and tree in a Pipeline\n",
        "pipeline = Pipeline(stages=[featureIndexer, dt])\n",
        "\n",
        "# Train model.  This also runs the indexer.\n",
        "model = pipeline.fit(trainingData)\n",
        "\n",
        "# Make predictions.\n",
        "predictions = model.transform(testData)\n",
        "\n",
        "# Select example rows to display.\n",
        "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
        "\n",
        "# Select (prediction, true label) and compute test error\n",
        "evaluator = RegressionEvaluator(\n",
        "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print (\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
        "\n",
        "treeModel = model.stages[1]\n",
        "print (treeModel) # summary only"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPJUJ-mAqGX1",
        "outputId": "2c018592-325c-4aea-8a18-45565ded8d5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+-----------+-----------------+---+----+----+--------+----------------+\n",
            "|Serial No.|GRE Score|TOEFL Score|University Rating|SOP|LOR |CGPA|Research|Chance of Admit |\n",
            "+----------+---------+-----------+-----------------+---+----+----+--------+----------------+\n",
            "|         1|      337|        118|                4|4.5| 4.5|9.65|       1|            0.92|\n",
            "|         2|      324|        107|                4|4.0| 4.5|8.87|       1|            0.76|\n",
            "|         3|      316|        104|                3|3.0| 3.5| 8.0|       1|            0.72|\n",
            "|         4|      322|        110|                3|3.5| 2.5|8.67|       1|             0.8|\n",
            "|         5|      314|        103|                2|2.0| 3.0|8.21|       0|            0.65|\n",
            "+----------+---------+-----------+-----------------+---+----+----+--------+----------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+--------------------+-----+\n",
            "|            features|label|\n",
            "+--------------------+-----+\n",
            "|[337.0,118.0,4.0,...| 0.92|\n",
            "|[324.0,107.0,4.0,...| 0.76|\n",
            "|[316.0,104.0,3.0,...| 0.72|\n",
            "|[322.0,110.0,3.0,...|  0.8|\n",
            "|[314.0,103.0,2.0,...| 0.65|\n",
            "+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "+------------------+-----+--------------------+\n",
            "|        prediction|label|            features|\n",
            "+------------------+-----+--------------------+\n",
            "|0.5022222222222221| 0.47|[290.0,100.0,1.0,...|\n",
            "|0.5054545454545454| 0.64|[293.0,97.0,2.0,2...|\n",
            "|0.5054545454545454| 0.46|[294.0,93.0,1.0,1...|\n",
            "|0.5054545454545454| 0.46|[295.0,93.0,1.0,2...|\n",
            "|0.5022222222222221| 0.47|[296.0,99.0,2.0,3...|\n",
            "+------------------+-----+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Root Mean Squared Error (RMSE) on test data = 0.0654712\n",
            "DecisionTreeRegressionModel: uid=DecisionTreeRegressor_112b291e022a, depth=5, numNodes=61, numFeatures=7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# option 2: Milk Grade Prediction\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.mllib.util import MLUtils\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.sql import Row\n",
        "from pyspark.ml.linalg import Vectors\n",
        "import pandas as pd\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "# create SparkSession\n",
        "spark = SparkSession.builder.appName(\"milknew\").getOrCreate()\n",
        "\n",
        "# load CSV to Spark DataFrame\n",
        "file_path = \"milknew.csv\"\n",
        "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "\n",
        "df.show(5)\n",
        "\n",
        "# Specify the feature columns and the label column\n",
        "feature_columns = [\"pH\", \"Temprature\", \"Taste\", \"Odor\", \"Fat \", \"Turbidity\", \"Colour\"]\n",
        "# label_column = \"Grade\"\n",
        "\n",
        "# Use VectorAssembler to combine feature columns into a single vector column\n",
        "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
        "\n",
        "# Transform the DataFrame\n",
        "df_transformed = assembler.transform(df)\n",
        "\n",
        "# Select only the features and label columns\n",
        "data = df_transformed.select(\"features\", label_column)\n",
        "data = data.withColumnRenamed(label_column, \"label\")\n",
        "\n",
        "data.show(5)\n",
        "\n",
        "# Index labels, adding metadata to the label column.\n",
        "# Fit on whole dataset to include all labels in index.\n",
        "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(data)\n",
        "# Automatically identify categorical features, and index them.\n",
        "# We specify maxCategories so features with > 4 distinct values are treated as continuous.\n",
        "featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
        "\n",
        "# Split the data into training and test sets (30% held out for testing)\n",
        "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
        "\n",
        "# Train a DecisionTree model.\n",
        "dt = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\n",
        "\n",
        "# Chain indexers and tree in a Pipeline\n",
        "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, dt])\n",
        "\n",
        "# Train model.  This also runs the indexers.\n",
        "model = pipeline.fit(trainingData)\n",
        "\n",
        "# Make predictions.\n",
        "predictions = model.transform(testData)\n",
        "\n",
        "# Select example rows to display.\n",
        "predictions.select(\"prediction\", \"indexedLabel\", \"features\").show(10)\n",
        "\n",
        "# Select (prediction, true label) and compute test error\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print (\"Test Error = %g\" % (1.0 - accuracy))\n",
        "\n",
        "treeModel = model.stages[2]\n",
        "print (treeModel) # summary only"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ed55JQkC6Se1",
        "outputId": "ff41a388-9454-4c7a-d7a7-ac0503e42863"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+-----+----+----+---------+------+------+\n",
            "| pH|Temprature|Taste|Odor|Fat |Turbidity|Colour| Grade|\n",
            "+---+----------+-----+----+----+---------+------+------+\n",
            "|6.6|        35|    1|   0|   1|        0|   254|  high|\n",
            "|6.6|        36|    0|   1|   0|        1|   253|  high|\n",
            "|8.5|        70|    1|   1|   1|        1|   246|   low|\n",
            "|9.5|        34|    1|   1|   0|        1|   255|   low|\n",
            "|6.6|        37|    0|   0|   0|        0|   255|medium|\n",
            "+---+----------+-----+----+----+---------+------+------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+--------------------+------+\n",
            "|            features| label|\n",
            "+--------------------+------+\n",
            "|[6.6,35.0,1.0,0.0...|  high|\n",
            "|[6.6,36.0,0.0,1.0...|  high|\n",
            "|[8.5,70.0,1.0,1.0...|   low|\n",
            "|[9.5,34.0,1.0,1.0...|   low|\n",
            "|(7,[0,1,6],[6.6,3...|medium|\n",
            "+--------------------+------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+----------+------------+--------------------+\n",
            "|prediction|indexedLabel|            features|\n",
            "+----------+------------+--------------------+\n",
            "|       1.0|         1.0|(7,[0,1,6],[6.5,3...|\n",
            "|       1.0|         1.0|(7,[0,1,6],[6.5,3...|\n",
            "|       1.0|         1.0|(7,[0,1,6],[6.5,3...|\n",
            "|       1.0|         1.0|(7,[0,1,6],[6.5,3...|\n",
            "|       1.0|         1.0|(7,[0,1,6],[6.5,3...|\n",
            "|       1.0|         1.0|(7,[0,1,6],[6.5,3...|\n",
            "|       1.0|         1.0|(7,[0,1,6],[6.5,3...|\n",
            "|       1.0|         1.0|(7,[0,1,6],[6.5,3...|\n",
            "|       1.0|         1.0|(7,[0,1,6],[6.5,3...|\n",
            "|       1.0|         1.0|(7,[0,1,6],[6.5,3...|\n",
            "+----------+------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "Test Error = 0.101974\n",
            "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_2ae6c1c6ef26, depth=5, numNodes=13, numClasses=3, numFeatures=7\n"
          ]
        }
      ]
    }
  ]
}